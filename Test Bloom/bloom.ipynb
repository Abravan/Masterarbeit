{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Drive ID  Recording ID Start (UTC) End (UTC)  Unnamed: 4 Day/Night  \\\n",
      "0         1             1    14:03:30  14:03:47         NaN       Day   \n",
      "1         2             1    12:00:29  12:00:54         NaN       Day   \n",
      "2         3             1    15:15:13  15:16:40         NaN     Night   \n",
      "3         3             2    15:21:50  15:24:12         NaN     Night   \n",
      "\n",
      "  Weather Conditions  Temperature[°C]  Humidity[%]  Precipitation[mm]  \n",
      "0          Clear sky             31.1           40                  0  \n",
      "1           Overcast             18.9           51                  0  \n",
      "2      Partly cloudy             -0.5           90                  0  \n",
      "3      Partly cloudy             -0.5           90                  0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "file_path = 'drive data.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fatem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load BLOOM model and tokenizer\n",
    "model_name = \"bigscience/bloom-560m\"  # Example model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt\n",
      "0  During the Day time, the weather was Clear sky...\n",
      "1  During the Day time, the weather was Overcast ...\n",
      "2  During the Night time, the weather was Partly ...\n",
      "3  During the Night time, the weather was Partly ...\n"
     ]
    }
   ],
   "source": [
    "# Combine columns into descriptive prompts\n",
    "def create_prompt(row):\n",
    "    return (f\"During the {row['Day/Night']} time, the weather was {row['Weather Conditions']} \"\n",
    "            f\"with a temperature of {row['Temperature[°C]']}°C, \"\n",
    "            f\"humidity at {row['Humidity[%]']}%, and precipitation of {row['Precipitation[mm]']} mm.\")\n",
    "\n",
    "data['prompt'] = data.apply(create_prompt, axis=1)\n",
    "print(data[['prompt']].head())  # Preview the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated descriptions saved to Bloom_result.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate text for each prompt\n",
    "generated_texts = []\n",
    "for prompt in data['prompt']:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "\n",
    "# Add generated texts back to the DataFrame\n",
    "data['generated_text'] = generated_texts\n",
    "\n",
    "# Save to a new CSV\n",
    "output_path = 'Bloom_result.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Generated descriptions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom-560m\"\n",
    "headers = {\"Authorization\": \"Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
